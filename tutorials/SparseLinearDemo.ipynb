{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparseLinear Demonstration using MNIST \n",
    "\n",
    "##### Training models consisting of sparsely connected linear layers\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Setup](#setup)\n",
    "- [Time and memory efficiency](#efficiency)\n",
    "- [Training with random inputs](#random)\n",
    "- [Training on MNIST](#mnist)\n",
    "- [Training sparse models with user-defined connections](#user)\n",
    "- [Training sparse models with dynamic connections](#dynamic)\n",
    "- [Training sparse models with small-world connections](#sw)\n",
    "- [Utilizing the activation sparsity feature](#activation)\n",
    "- [Training very wide and sparse models](#big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"intro\"></a>\n",
    "\n",
    "SparseLinear is a PyTorch package that allows a user to create extremely wide and sparse linear layers efficiently. A sparsely connected network is a network where each node is connected to some fraction of available nodes.\n",
    "\n",
    "The provided package is built on top of [PyTorch Sparse](https://github.com/rusty1s/pytorch_sparse), which provides optimized sparse matrix operations with autograd support in PyTorch.\n",
    "\n",
    "In this tutorial, we demonstrate its basic usage along with steps to train using the package features. Note that it is advisable to run these on the GPU instead of the CPU owing to much faster training times on the former."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"setup\"></a>\n",
    "\n",
    "We import PyTorch, which contains the (dense)linear module, and load the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the linear layer and demonstrate some of its built-in attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('in_features=10, out_features=20, bias=True',\n",
       " torch.Size([20, 10]),\n",
       " torch.Size([20]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1 = nn.Linear(10, 20)\n",
    "fc1.extra_repr(), fc1.weight.shape, fc1.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, we now import the `sparselinear` package. As can be observed, the custom layer's weights and biases can be accessed in the same manner as before. The new layer also returns some extra attributes about which we will discuss later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sparselinear as sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('in_features=10, out_features=20, bias=True, sparsity=0.9, connectivity=None, small_world=False',\n",
       " torch.Size([20, 10]),\n",
       " torch.Size([20]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl1 = sl.SparseLinear(10,20)\n",
    "sl1.extra_repr(), sl1.weight.shape, sl1.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take a look at the two weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 2.2700e-01,  1.9950e-04,  1.5138e-01, -2.3028e-01,  3.0498e-01,\n",
       "         -1.1592e-01,  8.9561e-03, -1.9644e-01, -1.8278e-01, -1.2167e-01],\n",
       "        [ 1.8054e-01,  4.9672e-03, -2.7930e-01,  1.7971e-02, -2.5313e-01,\n",
       "         -1.6389e-01,  2.8138e-02,  2.3216e-01,  8.5033e-02,  2.6193e-01],\n",
       "        [-1.3997e-01, -2.0780e-01, -1.3777e-01,  9.5758e-02, -1.1465e-01,\n",
       "         -3.0299e-01,  2.3639e-01,  2.3740e-01,  3.4879e-02, -2.8988e-01],\n",
       "        [ 3.4024e-02, -3.4284e-02, -3.1449e-01, -7.3634e-02,  1.0884e-01,\n",
       "          3.4649e-02,  2.2210e-01, -2.2692e-01,  1.7318e-01,  1.0567e-01],\n",
       "        [ 1.8497e-01,  8.6446e-02, -1.3994e-02, -1.8335e-01,  7.1342e-02,\n",
       "         -5.4367e-02, -1.2261e-01, -1.2711e-01,  1.2817e-01,  3.0136e-01],\n",
       "        [ 2.7756e-01, -2.6505e-01,  2.1932e-02,  2.2353e-01, -2.0779e-01,\n",
       "          2.9041e-01, -2.9108e-01,  2.5556e-02,  2.6355e-02,  9.2430e-02],\n",
       "        [-6.9308e-02,  1.4349e-01,  2.1799e-01,  9.2573e-02, -1.1946e-01,\n",
       "         -8.8171e-02,  1.8941e-01,  2.6366e-01, -2.2858e-01, -7.3599e-02],\n",
       "        [ 2.7514e-01,  5.0453e-02, -2.7328e-01,  1.8520e-01, -1.9852e-01,\n",
       "         -1.9735e-01,  2.4275e-01, -3.9498e-02, -9.1360e-02, -1.1861e-01],\n",
       "        [-1.1171e-01, -9.5010e-02,  8.9707e-02,  4.6313e-02,  1.4619e-01,\n",
       "          8.1823e-02,  1.7853e-01, -8.7963e-02, -1.1446e-01,  2.6627e-01],\n",
       "        [ 2.3615e-01, -3.6197e-02, -8.4897e-03, -1.6606e-01,  2.2675e-01,\n",
       "          2.4547e-01, -9.0289e-03,  2.4520e-01, -5.2978e-02,  2.1697e-01],\n",
       "        [-3.0513e-01,  1.3863e-01,  7.8270e-02,  1.4909e-01,  1.9973e-01,\n",
       "         -3.0507e-01,  2.5586e-01, -1.7424e-01, -1.5309e-01,  5.5867e-04],\n",
       "        [-7.0133e-02, -1.9032e-02, -1.2506e-01, -1.7848e-01, -2.9393e-02,\n",
       "         -1.2914e-01,  2.6337e-01,  2.6768e-02,  1.2672e-01,  2.7603e-01],\n",
       "        [-2.8693e-01, -2.3720e-01,  2.5797e-01,  1.4165e-01,  1.3373e-01,\n",
       "          1.8109e-01, -1.6957e-01, -1.7997e-02,  8.7014e-02, -2.4652e-01],\n",
       "        [-2.5947e-01, -2.8787e-01, -2.5539e-01,  1.6618e-01,  1.8108e-01,\n",
       "          2.5726e-01, -8.1458e-02, -5.7938e-02, -1.2392e-01,  2.4684e-01],\n",
       "        [-1.4422e-01,  2.5416e-01, -3.1530e-01, -3.4516e-02,  2.4788e-03,\n",
       "         -4.7461e-02, -4.3661e-02,  6.9811e-02, -1.5750e-01, -2.6075e-02],\n",
       "        [-2.1888e-02,  7.3086e-02, -1.8007e-01,  1.3642e-01,  3.5587e-02,\n",
       "         -9.6858e-02, -2.9306e-01,  3.6169e-02,  1.5752e-01, -1.6869e-01],\n",
       "        [-1.4439e-01,  1.7146e-01, -2.5298e-01, -8.1204e-02, -2.2055e-01,\n",
       "          7.1367e-02, -1.9483e-01,  6.2341e-02,  2.7298e-01, -1.9703e-01],\n",
       "        [ 2.4394e-01, -1.1722e-01,  1.9469e-01, -1.3386e-01,  2.3983e-02,\n",
       "         -1.6841e-01, -2.1754e-01, -3.1110e-01, -3.0143e-01, -2.0946e-01],\n",
       "        [-5.3311e-02, -2.9350e-01, -2.7600e-01, -2.5163e-02,  1.9405e-01,\n",
       "          2.2620e-01, -2.3203e-01,  2.2993e-01,  2.7201e-02, -2.3870e-01],\n",
       "        [-1.0589e-01,  2.1078e-01,  1.7010e-01,  1.1657e-01,  9.0184e-02,\n",
       "          2.8292e-01, -9.2838e-02,  6.3635e-02,  6.2464e-02, -3.0613e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  0,  1,  2,  3,  3,  4,  5,  5,  6,  7, 11, 12, 12,\n",
       "                        13, 14, 14, 15, 15, 19],\n",
       "                       [ 3,  8,  1,  4,  0,  7,  1,  5,  8,  7,  0,  2,  3,  4,\n",
       "                         7,  3,  7,  4,  7,  9]]),\n",
       "       values=tensor([-0.0633, -0.0889, -0.1319, -0.0976,  0.0678, -0.0097,\n",
       "                      -0.1309, -0.0588, -0.2626, -0.1929,  0.2060, -0.2528,\n",
       "                      -0.0253,  0.1744,  0.2165,  0.1699, -0.0991, -0.2581,\n",
       "                       0.3090,  0.0959]),\n",
       "       size=(20, 10), nnz=20, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the first weight matrix has 200 non-zero entries while the second one has 20 non-zero entries as specified by `nnz`. The indices tensor keeps track of all the indices where a non-zero entry is present with the corresponding entry in the values tensor providing the entry at that index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and Memory Efficiency <a name=\"efficiency\"></a>\n",
    "\n",
    "The `SparseLinear` class is ideal for very wide and sparse layers. Since we utilize sparse tensors and only store non-zero values (and their corresponding indices), `SparseLinear` is much more efficient in terms of memory consumption than simply applying a mask over a standard dense weight matrix -- as is often done by researchers and practioners. Since we only perform computations on non-zero values, we see speedups in computation time for large matrices as well. As hardware becomes more well-suited for sparse computations, these speedups will likely increase.\n",
    "\n",
    "To show this, we create a (20000, 20000) `SparseLinear` layer with 99% sparsity and compare its runtime to that of a standard `Linear` layer. Later in this notebook, we create massive layers that would not be possible with standard `Linear` layers due to memory inefficiencies.\n",
    "\n",
    "We initialize two layers and define the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl2 = sl.SparseLinear(20000, 20000, sparsity=.99).cuda()\n",
    "\n",
    "# Reduce weight dimensions if memory errors are raised\n",
    "fc2 = nn.Linear(20000, 20000).cuda()\n",
    "\n",
    "x = torch.rand(20000, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We time the inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.85 ms ± 93.1 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit y = sl2(x)\n",
    "%timeit y = fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We time the training step for SparseLinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789 µs ± 666 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y = sl2(x)\n",
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We time the training step for Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.29 ms ± 86 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y = fc2(x)\n",
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete layers to save GPU memory when running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sl2, fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with random inputs <a name=\"random\"></a>\n",
    "\n",
    "Next, we demonstrate how to train a two-layer network using the `SparseLinear` module provided in the package. The code has been built upon the PyTorch [tutorial](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html) to highlight the parallels between the `nn.Linear` and `sl.SparseLinear` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense model loss: 4.417; Sparse model loss: 5.111\n",
      "Dense model loss: 0.075; Sparse model loss: 0.031\n",
      "Dense model loss: 0.002; Sparse model loss: 0.000\n",
      "Dense model loss: 0.000; Sparse model loss: 0.000\n",
      "Dense model loss: 0.000; Sparse model loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 200, 1000, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our dense model as a sequence of layers. \n",
    "model_dense = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# Use the sl package to define our sparse model as a sequence of layers.\n",
    "# Note that the default sparsity is 90%.\n",
    "model_sparse = torch.nn.Sequential(\n",
    "    sl.SparseLinear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    sl.SparseLinear(H, D_out),\n",
    ")\n",
    "\n",
    "# We will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# We define our learning rates.\n",
    "# Note that sparse and dense models may require different learning rates.\n",
    "learning_rate_dense = 1e-4\n",
    "learning_rate_sparse = 1e-3\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass\n",
    "    y_pred_dense = model_dense(x)\n",
    "    y_pred_sparse = model_sparse(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss_dense = loss_fn(y_pred_dense, y)\n",
    "    loss_sparse = loss_fn(y_pred_sparse, y)\n",
    "    if t % 100 == 99:\n",
    "        print(\"Dense model loss: %.3f; Sparse model loss: %.3f\" %(loss_dense.item(), loss_sparse.item()))\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model_dense.zero_grad()\n",
    "    model_sparse.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss_dense.backward()\n",
    "    loss_sparse.backward()\n",
    "\n",
    "    # Update the weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        for param in model_dense.parameters():\n",
    "            param -= learning_rate_dense * param.grad\n",
    "            \n",
    "        for param in model_sparse.parameters():\n",
    "            param -= learning_rate_sparse * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the loss value in both models decreases. Let's now build models using this module and train on the MNIST digit classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on MNIST <a name=\"mnist\"></a>\n",
    "\n",
    "We start by doing the initial imports, generating transforms, creating the dataset along with the dataloader, defining the loss function and some other helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "batch_size = 64\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=tf)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=tf)\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_dataloader, test_dataloader, num_epochs=20):\n",
    "    since = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        cum_loss, total, correct = 0, 0, 0\n",
    "        model.train()\n",
    "        \n",
    "        # Training epoch\n",
    "        for i, (images, labels) in enumerate(train_dataloader, 0):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass & statistics\n",
    "            out = model(images)\n",
    "            predicted = out.argmax(dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss = criterion(out, labels)\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "            # Backwards pass & update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        epoch_loss = images.shape[0] * cum_loss / total\n",
    "        epoch_acc = 100 * (correct / total)\n",
    "        print('Epoch %d' % (epoch + 1))\n",
    "        print('Training Loss: {:.4f}; Training Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        \n",
    "        cum_loss, total, correct = 0, 0, 0\n",
    "        model.eval()\n",
    "        \n",
    "        # Test epoch\n",
    "        for i, (images, labels) in enumerate(test_dataloader, 0):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass & statistics\n",
    "            out = model(images)\n",
    "            predicted = out.argmax(dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss = criterion(out, labels)\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = images.shape[0] * cum_loss / total\n",
    "        epoch_acc = 100 * (correct / total)\n",
    "        \n",
    "        print('Test Loss: {:.4f}; Test Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        print('------------')\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "\tN = x.shape[0]\n",
    "\treturn x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a dense model\n",
    "\n",
    "We start off with training a two-layer fully connected network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "\tFlatten(),\n",
    "\tnn.Linear(784, 2000),\n",
    "    nn.LayerNorm(2000),\n",
    "\tnn.ReLU(),\n",
    "    nn.Linear(2000, 10),\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we set everything up, we declare the optimizer and start training the dense model. We use SGD as the optimizer since we found its behavior to be slightly better than that of others. However, one is free to choose any optimizer as long as there exists an implementation for it to handle sparse tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2062; Training Acc: 93.5916\n",
      "Test Loss: 0.1250; Test Acc: 95.9736\n",
      "------------\n",
      "Epoch 2\n",
      "Training Loss: 0.0778; Training Acc: 97.6387\n",
      "Test Loss: 0.0949; Test Acc: 96.9351\n",
      "------------\n",
      "Epoch 3\n",
      "Training Loss: 0.0491; Training Acc: 98.4775\n",
      "Test Loss: 0.0785; Test Acc: 97.5160\n",
      "------------\n",
      "Epoch 4\n",
      "Training Loss: 0.0311; Training Acc: 99.0678\n",
      "Test Loss: 0.0664; Test Acc: 97.9667\n",
      "------------\n",
      "Epoch 5\n",
      "Training Loss: 0.0201; Training Acc: 99.4614\n",
      "Test Loss: 0.0704; Test Acc: 97.7564\n",
      "------------\n",
      "Epoch 6\n",
      "Training Loss: 0.0131; Training Acc: 99.7065\n",
      "Test Loss: 0.0549; Test Acc: 98.3173\n",
      "------------\n",
      "Epoch 7\n",
      "Training Loss: 0.0081; Training Acc: 99.8749\n",
      "Test Loss: 0.0592; Test Acc: 98.2372\n",
      "------------\n",
      "Epoch 8\n",
      "Training Loss: 0.0057; Training Acc: 99.9350\n",
      "Test Loss: 0.0536; Test Acc: 98.3674\n",
      "------------\n",
      "Epoch 9\n",
      "Training Loss: 0.0035; Training Acc: 99.9850\n",
      "Test Loss: 0.0551; Test Acc: 98.3173\n",
      "------------\n",
      "Epoch 10\n",
      "Training Loss: 0.0025; Training Acc: 99.9983\n",
      "Test Loss: 0.0518; Test Acc: 98.4976\n",
      "------------\n",
      "Epoch 11\n",
      "Training Loss: 0.0020; Training Acc: 100.0000\n",
      "Test Loss: 0.0526; Test Acc: 98.3974\n",
      "------------\n",
      "Epoch 12\n",
      "Training Loss: 0.0017; Training Acc: 99.9983\n",
      "Test Loss: 0.0530; Test Acc: 98.4976\n",
      "------------\n",
      "Epoch 13\n",
      "Training Loss: 0.0015; Training Acc: 100.0000\n",
      "Test Loss: 0.0531; Test Acc: 98.5377\n",
      "------------\n",
      "Epoch 14\n",
      "Training Loss: 0.0013; Training Acc: 100.0000\n",
      "Test Loss: 0.0532; Test Acc: 98.5076\n",
      "------------\n",
      "Epoch 15\n",
      "Training Loss: 0.0012; Training Acc: 100.0000\n",
      "Test Loss: 0.0544; Test Acc: 98.4675\n",
      "------------\n",
      "Epoch 16\n",
      "Training Loss: 0.0011; Training Acc: 100.0000\n",
      "Test Loss: 0.0539; Test Acc: 98.4776\n",
      "------------\n",
      "Epoch 17\n",
      "Training Loss: 0.0010; Training Acc: 100.0000\n",
      "Test Loss: 0.0537; Test Acc: 98.5276\n",
      "------------\n",
      "Epoch 18\n",
      "Training Loss: 0.0009; Training Acc: 100.0000\n",
      "Test Loss: 0.0546; Test Acc: 98.4976\n",
      "------------\n",
      "Epoch 19\n",
      "Training Loss: 0.0009; Training Acc: 100.0000\n",
      "Test Loss: 0.0545; Test Acc: 98.5076\n",
      "------------\n",
      "Epoch 20\n",
      "Training Loss: 0.0008; Training Acc: 100.0000\n",
      "Test Loss: 0.0544; Test Acc: 98.4776\n",
      "------------\n",
      "\n",
      "Training complete in 4m 56s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "#Perform the training \n",
    "train_model(model, optimizer, criterion, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Sparse Model\n",
    "\n",
    "(with the default configuration)\n",
    "\n",
    "In the same way that we declared a dense model, we now declare a sparse model with the same number of input and output features but far fewer parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model = nn.Sequential(\n",
    "\tFlatten(),\n",
    "\tsl.SparseLinear(784, 2000),\n",
    "    nn.LayerNorm(2000),\n",
    "\tnn.ReLU(),\n",
    "    sl.SparseLinear(2000, 10)\n",
    ")\n",
    "sparse_model = sparse_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train this model. Note that the learning rate is an order of magnitude higher. This is something we have found to be a rule of thumb while training these models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2103; Training Acc: 93.6383\n",
      "Test Loss: 0.1184; Test Acc: 96.3141\n",
      "------------\n",
      "Epoch 2\n",
      "Training Loss: 0.0896; Training Acc: 97.2269\n",
      "Test Loss: 0.1022; Test Acc: 96.7949\n",
      "------------\n",
      "Epoch 3\n",
      "Training Loss: 0.0604; Training Acc: 98.0340\n",
      "Test Loss: 0.0880; Test Acc: 97.4159\n",
      "------------\n",
      "Epoch 4\n",
      "Training Loss: 0.0424; Training Acc: 98.6860\n",
      "Test Loss: 0.0799; Test Acc: 97.6262\n",
      "------------\n",
      "Epoch 5\n",
      "Training Loss: 0.0313; Training Acc: 99.0061\n",
      "Test Loss: 0.0782; Test Acc: 97.7764\n",
      "------------\n",
      "Epoch 6\n",
      "Training Loss: 0.0228; Training Acc: 99.2696\n",
      "Test Loss: 0.0843; Test Acc: 97.6963\n",
      "------------\n",
      "Epoch 7\n",
      "Training Loss: 0.0163; Training Acc: 99.5147\n",
      "Test Loss: 0.0861; Test Acc: 97.8466\n",
      "------------\n",
      "Epoch 8\n",
      "Training Loss: 0.0117; Training Acc: 99.6565\n",
      "Test Loss: 0.0917; Test Acc: 97.7364\n",
      "------------\n",
      "Epoch 9\n",
      "Training Loss: 0.0074; Training Acc: 99.8332\n",
      "Test Loss: 0.0913; Test Acc: 97.8666\n",
      "------------\n",
      "Epoch 10\n",
      "Training Loss: 0.0044; Training Acc: 99.9200\n",
      "Test Loss: 0.0819; Test Acc: 97.9667\n",
      "------------\n",
      "Epoch 11\n",
      "Training Loss: 0.0023; Training Acc: 99.9800\n",
      "Test Loss: 0.0852; Test Acc: 98.0970\n",
      "------------\n",
      "Epoch 12\n",
      "Training Loss: 0.0012; Training Acc: 99.9950\n",
      "Test Loss: 0.0894; Test Acc: 98.0469\n",
      "------------\n",
      "Epoch 13\n",
      "Training Loss: 0.0008; Training Acc: 99.9967\n",
      "Test Loss: 0.0864; Test Acc: 98.1871\n",
      "------------\n",
      "Epoch 14\n",
      "Training Loss: 0.0006; Training Acc: 100.0000\n",
      "Test Loss: 0.0874; Test Acc: 98.1470\n",
      "------------\n",
      "Epoch 15\n",
      "Training Loss: 0.0005; Training Acc: 100.0000\n",
      "Test Loss: 0.0884; Test Acc: 98.1270\n",
      "------------\n",
      "Epoch 16\n",
      "Training Loss: 0.0004; Training Acc: 100.0000\n",
      "Test Loss: 0.0891; Test Acc: 98.1571\n",
      "------------\n",
      "Epoch 17\n",
      "Training Loss: 0.0004; Training Acc: 100.0000\n",
      "Test Loss: 0.0901; Test Acc: 98.1470\n",
      "------------\n",
      "Epoch 18\n",
      "Training Loss: 0.0004; Training Acc: 100.0000\n",
      "Test Loss: 0.0910; Test Acc: 98.1070\n",
      "------------\n",
      "Epoch 19\n",
      "Training Loss: 0.0003; Training Acc: 100.0000\n",
      "Test Loss: 0.0916; Test Acc: 98.0970\n",
      "------------\n",
      "Epoch 20\n",
      "Training Loss: 0.0003; Training Acc: 100.0000\n",
      "Test Loss: 0.0918; Test Acc: 98.1571\n",
      "------------\n",
      "\n",
      "Training complete in 5m 12s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = optim.SGD(sparse_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "train_model(sparse_model, optimizer, criterion, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the two models perform comparably.\n",
    "\n",
    "\n",
    "However, while the dense model has a total of 1590010 parameters, the sparse model only has 160810 parameters. This translates to **~89.8%** parameter reduction in the sparse model! \n",
    "\n",
    "We display the weight parameter counts of the two layers below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([156800]),\n",
       " torch.Size([2000, 784]),\n",
       " torch.Size([10, 2000]),\n",
       " torch.Size([2000]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_model[1].weights.shape, model[1].weight.shape, model[4].weight.shape, sparse_model[4].weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sparse models with user-defined connections <a name=\"user\"></a>\n",
    "\n",
    "Instead of using the random set of connections created during initialization between the input and output neurons, one can choose to define one's own connections to the sparse linear layer by providing an input long tensor of shape (2,`nnz`) specifying connections from input to output neurons using the `connectivity` argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a connectivity matrix where the input layer is connected to random entries in the output layer. Of course, this is just a small demonstration and one can experiment here with different connectivity matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_connections = 200\n",
    "col = torch.arange(784).repeat_interleave(num_connections).view(1,-1).long()\n",
    "row = torch.randint(low=0, high=2000, size=(784*num_connections,)).view(1,-1).long()\n",
    "connections = torch.cat((row, col), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide our connectivity matrix as an input to the `SparseLinear` module and follow the same training procedure as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model_user = nn.Sequential(\n",
    "\tFlatten(),\n",
    "\tsl.SparseLinear(784, 2000, connectivity=connections),\n",
    "    nn.LayerNorm(2000),\n",
    "\tnn.ReLU(),\n",
    "    sl.SparseLinear(2000, 10)\n",
    ")\n",
    "sparse_model_user = sparse_model_user.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2136; Training Acc: 93.6283\n",
      "Test Loss: 0.1089; Test Acc: 96.8249\n",
      "------------\n",
      "Epoch 2\n",
      "Training Loss: 0.0902; Training Acc: 97.3302\n",
      "Test Loss: 0.1025; Test Acc: 96.8049\n",
      "------------\n",
      "Epoch 3\n",
      "Training Loss: 0.0619; Training Acc: 98.0606\n",
      "Test Loss: 0.0877; Test Acc: 97.3658\n",
      "------------\n",
      "Epoch 4\n",
      "Training Loss: 0.0437; Training Acc: 98.5859\n",
      "Test Loss: 0.0814; Test Acc: 97.5160\n",
      "------------\n",
      "Epoch 5\n",
      "Training Loss: 0.0340; Training Acc: 98.8661\n",
      "Test Loss: 0.0854; Test Acc: 97.5461\n",
      "------------\n",
      "Epoch 6\n",
      "Training Loss: 0.0239; Training Acc: 99.2496\n",
      "Test Loss: 0.0815; Test Acc: 97.6062\n",
      "------------\n",
      "Epoch 7\n",
      "Training Loss: 0.0165; Training Acc: 99.4997\n",
      "Test Loss: 0.0901; Test Acc: 97.5060\n",
      "------------\n",
      "Epoch 8\n",
      "Training Loss: 0.0122; Training Acc: 99.6298\n",
      "Test Loss: 0.0873; Test Acc: 97.7063\n",
      "------------\n",
      "Epoch 9\n",
      "Training Loss: 0.0080; Training Acc: 99.8016\n",
      "Test Loss: 0.0917; Test Acc: 97.6963\n",
      "------------\n",
      "Epoch 10\n",
      "Training Loss: 0.0041; Training Acc: 99.9450\n",
      "Test Loss: 0.0837; Test Acc: 97.8766\n",
      "------------\n",
      "Epoch 11\n",
      "Training Loss: 0.0020; Training Acc: 99.9833\n",
      "Test Loss: 0.0864; Test Acc: 97.8566\n",
      "------------\n",
      "Epoch 12\n",
      "Training Loss: 0.0012; Training Acc: 99.9967\n",
      "Test Loss: 0.0852; Test Acc: 97.9267\n",
      "------------\n",
      "Epoch 13\n",
      "Training Loss: 0.0008; Training Acc: 100.0000\n",
      "Test Loss: 0.0849; Test Acc: 97.9167\n",
      "------------\n",
      "Epoch 14\n",
      "Training Loss: 0.0006; Training Acc: 100.0000\n",
      "Test Loss: 0.0871; Test Acc: 97.9167\n",
      "------------\n",
      "Epoch 15\n",
      "Training Loss: 0.0005; Training Acc: 100.0000\n",
      "Test Loss: 0.0881; Test Acc: 97.8766\n",
      "------------\n",
      "Epoch 16\n",
      "Training Loss: 0.0005; Training Acc: 100.0000\n",
      "Test Loss: 0.0884; Test Acc: 97.8966\n",
      "------------\n",
      "Epoch 17\n",
      "Training Loss: 0.0004; Training Acc: 100.0000\n",
      "Test Loss: 0.0896; Test Acc: 97.8866\n",
      "------------\n",
      "Epoch 18\n",
      "Training Loss: 0.0004; Training Acc: 100.0000\n",
      "Test Loss: 0.0903; Test Acc: 97.8966\n",
      "------------\n",
      "Epoch 19\n",
      "Training Loss: 0.0004; Training Acc: 100.0000\n",
      "Test Loss: 0.0905; Test Acc: 97.8966\n",
      "------------\n",
      "Epoch 20\n",
      "Training Loss: 0.0003; Training Acc: 100.0000\n",
      "Test Loss: 0.0918; Test Acc: 97.8766\n",
      "------------\n",
      "\n",
      "Training complete in 5m 19s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = optim.SGD(sparse_model_user.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "train_model(sparse_model_user, optimizer, criterion, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sparse model with dynamic connections <a name=\"dynamic\"></a>\n",
    "\n",
    "The default `SparseLinear` model creates a random set of connections during initialization between the input and output neurons. An improvement over this strategy is to prune some non-required connections and grow (hopefully)required ones. We implement the [Rigging the Lottery](https://arxiv.org/pdf/1911.11134.pdf) algorithm to achieve this. Specifying `dynamic` to be `True` alters the layer connections dynamically while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparse_model_dynamic = nn.Sequential(\n",
    "\tFlatten(),\n",
    "\tsl.SparseLinear(784, 2000, dynamic=True),\n",
    "    nn.LayerNorm(2000),\n",
    "\tnn.ReLU(),\n",
    "    sl.SparseLinear(2000, 10, dynamic=True)\n",
    ")\n",
    "sparse_model_dynamic = sparse_model_dynamic.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2137; Training Acc: 93.5616\n",
      "Test Loss: 0.1246; Test Acc: 96.0737\n",
      "------------\n",
      "Epoch 2\n",
      "Training Loss: 0.0915; Training Acc: 97.2269\n",
      "Test Loss: 0.0815; Test Acc: 97.4960\n",
      "------------\n",
      "Epoch 3\n",
      "Training Loss: 0.0613; Training Acc: 98.1057\n",
      "Test Loss: 0.0907; Test Acc: 97.2155\n",
      "------------\n",
      "Epoch 4\n",
      "Training Loss: 0.0452; Training Acc: 98.5676\n",
      "Test Loss: 0.0839; Test Acc: 97.3658\n",
      "------------\n",
      "Epoch 5\n",
      "Training Loss: 0.0336; Training Acc: 98.9695\n",
      "Test Loss: 0.0789; Test Acc: 97.5761\n",
      "------------\n",
      "Epoch 6\n",
      "Training Loss: 0.0219; Training Acc: 99.3647\n",
      "Test Loss: 0.0681; Test Acc: 97.8365\n",
      "------------\n",
      "Epoch 7\n",
      "Training Loss: 0.0128; Training Acc: 99.7182\n",
      "Test Loss: 0.0670; Test Acc: 97.9267\n",
      "------------\n",
      "Epoch 8\n",
      "Training Loss: 0.0116; Training Acc: 99.7665\n",
      "Test Loss: 0.0672; Test Acc: 97.9267\n",
      "------------\n",
      "Epoch 9\n",
      "Training Loss: 0.0110; Training Acc: 99.7816\n",
      "Test Loss: 0.0667; Test Acc: 97.9367\n",
      "------------\n",
      "Epoch 10\n",
      "Training Loss: 0.0106; Training Acc: 99.7932\n",
      "Test Loss: 0.0669; Test Acc: 97.9567\n",
      "------------\n",
      "Epoch 11\n",
      "Training Loss: 0.0102; Training Acc: 99.8032\n",
      "Test Loss: 0.0672; Test Acc: 97.9667\n",
      "------------\n",
      "Epoch 12\n",
      "Training Loss: 0.0100; Training Acc: 99.8299\n",
      "Test Loss: 0.0673; Test Acc: 98.0068\n",
      "------------\n",
      "Epoch 13\n",
      "Training Loss: 0.0097; Training Acc: 99.8199\n",
      "Test Loss: 0.0674; Test Acc: 97.9968\n",
      "------------\n",
      "Epoch 14\n",
      "Training Loss: 0.0095; Training Acc: 99.8332\n",
      "Test Loss: 0.0677; Test Acc: 98.0068\n",
      "------------\n",
      "Epoch 15\n",
      "Training Loss: 0.0093; Training Acc: 99.8432\n",
      "Test Loss: 0.0677; Test Acc: 98.0068\n",
      "------------\n",
      "Epoch 16\n",
      "Training Loss: 0.0092; Training Acc: 99.8466\n",
      "Test Loss: 0.0679; Test Acc: 97.9868\n",
      "------------\n",
      "Epoch 17\n",
      "Training Loss: 0.0090; Training Acc: 99.8483\n",
      "Test Loss: 0.0685; Test Acc: 98.0469\n",
      "------------\n",
      "Epoch 18\n",
      "Training Loss: 0.0089; Training Acc: 99.8466\n",
      "Test Loss: 0.0687; Test Acc: 97.9768\n",
      "------------\n",
      "Epoch 19\n",
      "Training Loss: 0.0088; Training Acc: 99.8599\n",
      "Test Loss: 0.0685; Test Acc: 98.0168\n",
      "------------\n",
      "Epoch 20\n",
      "Training Loss: 0.0087; Training Acc: 99.8533\n",
      "Test Loss: 0.0689; Test Acc: 98.0469\n",
      "------------\n",
      "\n",
      "Training complete in 5m 15s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-3\n",
    "optimizer = optim.SGD(sparse_model_dynamic.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_model(sparse_model_dynamic, optimizer, criterion, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sparse model with small-world connections <a name=\"sw\"></a>\n",
    "\n",
    "Some sparsity patterns tend to perform better than others. Small-world sparsity provides a network that is mostly locally connected with a few global, long-range connections scattered in. See [here](https://en.wikipedia.org/wiki/Small-world_network). We implement an initialization strategy to incorporate small-world sparsity in the model. To specify, set `small_world` to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparse_model_sw = nn.Sequential(\n",
    "\tFlatten(),\n",
    "\tsl.SparseLinear(784, 2000, small_world=True),\n",
    "    nn.LayerNorm(2000),\n",
    "\tnn.ReLU(),\n",
    "    sl.SparseLinear(2000, 10, small_world=True)\n",
    ")\n",
    "sparse_model_sw = sparse_model_sw.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2043; Training Acc: 93.7817\n",
      "Test Loss: 0.1040; Test Acc: 96.7748\n",
      "------------\n",
      "Epoch 2\n",
      "Training Loss: 0.0856; Training Acc: 97.3202\n",
      "Test Loss: 0.0853; Test Acc: 97.2756\n",
      "------------\n",
      "Epoch 3\n",
      "Training Loss: 0.0573; Training Acc: 98.2057\n",
      "Test Loss: 0.0816; Test Acc: 97.5661\n",
      "------------\n",
      "Epoch 4\n",
      "Training Loss: 0.0404; Training Acc: 98.7176\n",
      "Test Loss: 0.0786; Test Acc: 97.5761\n",
      "------------\n",
      "Epoch 5\n",
      "Training Loss: 0.0297; Training Acc: 99.0211\n",
      "Test Loss: 0.0741; Test Acc: 97.7464\n",
      "------------\n",
      "Epoch 6\n",
      "Training Loss: 0.0211; Training Acc: 99.3246\n",
      "Test Loss: 0.0740; Test Acc: 97.9267\n",
      "------------\n",
      "Epoch 7\n",
      "Training Loss: 0.0162; Training Acc: 99.4597\n",
      "Test Loss: 0.0808; Test Acc: 97.8666\n",
      "------------\n",
      "Epoch 8\n",
      "Training Loss: 0.0108; Training Acc: 99.7132\n",
      "Test Loss: 0.0900; Test Acc: 97.6462\n",
      "------------\n",
      "Epoch 9\n",
      "Training Loss: 0.0090; Training Acc: 99.7148\n",
      "Test Loss: 0.0858; Test Acc: 97.8666\n",
      "------------\n",
      "Epoch 10\n",
      "Training Loss: 0.0055; Training Acc: 99.8666\n",
      "Test Loss: 0.0830; Test Acc: 98.0469\n",
      "------------\n",
      "Epoch 11\n",
      "Training Loss: 0.0029; Training Acc: 99.9516\n",
      "Test Loss: 0.0785; Test Acc: 98.0068\n",
      "------------\n",
      "Epoch 12\n",
      "Training Loss: 0.0013; Training Acc: 99.9950\n",
      "Test Loss: 0.0808; Test Acc: 98.2472\n",
      "------------\n",
      "Epoch 13\n",
      "Training Loss: 0.0007; Training Acc: 100.0000\n",
      "Test Loss: 0.0796; Test Acc: 98.2071\n",
      "------------\n",
      "Epoch 14\n",
      "Training Loss: 0.0005; Training Acc: 100.0000\n",
      "Test Loss: 0.0811; Test Acc: 98.1571\n",
      "------------\n",
      "Epoch 15\n",
      "Training Loss: 0.0005; Training Acc: 100.0000\n",
      "Test Loss: 0.0823; Test Acc: 98.1871\n",
      "------------\n",
      "Epoch 16\n",
      "Training Loss: 0.0004; Training Acc: 100.0000\n",
      "Test Loss: 0.0827; Test Acc: 98.1871\n",
      "------------\n",
      "Epoch 17\n",
      "Training Loss: 0.0004; Training Acc: 100.0000\n",
      "Test Loss: 0.0841; Test Acc: 98.2071\n",
      "------------\n",
      "Epoch 18\n",
      "Training Loss: 0.0003; Training Acc: 100.0000\n",
      "Test Loss: 0.0844; Test Acc: 98.1671\n",
      "------------\n",
      "Epoch 19\n",
      "Training Loss: 0.0003; Training Acc: 100.0000\n",
      "Test Loss: 0.0849; Test Acc: 98.1871\n",
      "------------\n",
      "Epoch 20\n",
      "Training Loss: 0.0003; Training Acc: 100.0000\n",
      "Test Loss: 0.0854; Test Acc: 98.2171\n",
      "------------\n",
      "\n",
      "Training complete in 5m 21s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = optim.SGD(sparse_model_sw.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_model(sparse_model_sw, optimizer, criterion, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the activation sparsity feature <a name=\"activation\"></a>\n",
    "\n",
    "The `SparseLinear` layer is constructed for parameter sparsity; however, we make no stipulations on the sparsity (or density) of the activations. We include an option for sparse activations using the K-Winners strategy. This paper describes a potential method ([k-winners](https://arxiv.org/pdf/1903.11257.pdf) layer) which we use to train both linear and sparse linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import activationsparsity as asy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train a linear model using this activation sparsity feature. By default, we set `act_sparsity=0.65` (which means `k=(1-0.65)*2000`) for the layer below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_asy = nn.Sequential(\n",
    "\tFlatten(),\n",
    "    nn.Linear(784, 2000),\n",
    "    nn.LayerNorm(2000),\n",
    "    asy.ActivationSparsity(),\n",
    "    nn.Linear(2000,10)\n",
    ")\n",
    "model_asy = model_asy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2241; Training Acc: 93.3548\n",
      "Test Loss: 0.1153; Test Acc: 96.5845\n",
      "------------\n",
      "Epoch 2\n",
      "Training Loss: 0.0901; Training Acc: 97.3102\n",
      "Test Loss: 0.0894; Test Acc: 97.1855\n",
      "------------\n",
      "Epoch 3\n",
      "Training Loss: 0.0590; Training Acc: 98.3825\n",
      "Test Loss: 0.0785; Test Acc: 97.6462\n",
      "------------\n",
      "Epoch 4\n",
      "Training Loss: 0.0415; Training Acc: 98.8877\n",
      "Test Loss: 0.0671; Test Acc: 97.8866\n",
      "------------\n",
      "Epoch 5\n",
      "Training Loss: 0.0303; Training Acc: 99.2529\n",
      "Test Loss: 0.0616; Test Acc: 98.0569\n",
      "------------\n",
      "Epoch 6\n",
      "Training Loss: 0.0222; Training Acc: 99.5114\n",
      "Test Loss: 0.0602; Test Acc: 98.1270\n",
      "------------\n",
      "Epoch 7\n",
      "Training Loss: 0.0168; Training Acc: 99.6748\n",
      "Test Loss: 0.0611; Test Acc: 98.0569\n",
      "------------\n",
      "Epoch 8\n",
      "Training Loss: 0.0128; Training Acc: 99.8366\n",
      "Test Loss: 0.0604; Test Acc: 98.0970\n",
      "------------\n",
      "Epoch 9\n",
      "Training Loss: 0.0102; Training Acc: 99.8849\n",
      "Test Loss: 0.0569; Test Acc: 98.2472\n",
      "------------\n",
      "Epoch 10\n",
      "Training Loss: 0.0079; Training Acc: 99.9400\n",
      "Test Loss: 0.0581; Test Acc: 98.1971\n",
      "------------\n",
      "Epoch 11\n",
      "Training Loss: 0.0065; Training Acc: 99.9633\n",
      "Test Loss: 0.0559; Test Acc: 98.3173\n",
      "------------\n",
      "Epoch 12\n",
      "Training Loss: 0.0053; Training Acc: 99.9733\n",
      "Test Loss: 0.0538; Test Acc: 98.3874\n",
      "------------\n",
      "Epoch 13\n",
      "Training Loss: 0.0045; Training Acc: 99.9917\n",
      "Test Loss: 0.0543; Test Acc: 98.3474\n",
      "------------\n",
      "Epoch 14\n",
      "Training Loss: 0.0040; Training Acc: 99.9950\n",
      "Test Loss: 0.0576; Test Acc: 98.3073\n",
      "------------\n",
      "Epoch 15\n",
      "Training Loss: 0.0036; Training Acc: 99.9900\n",
      "Test Loss: 0.0555; Test Acc: 98.3273\n",
      "------------\n",
      "Epoch 16\n",
      "Training Loss: 0.0032; Training Acc: 99.9983\n",
      "Test Loss: 0.0556; Test Acc: 98.3974\n",
      "------------\n",
      "Epoch 17\n",
      "Training Loss: 0.0029; Training Acc: 99.9967\n",
      "Test Loss: 0.0581; Test Acc: 98.3574\n",
      "------------\n",
      "Epoch 18\n",
      "Training Loss: 0.0026; Training Acc: 100.0000\n",
      "Test Loss: 0.0577; Test Acc: 98.2472\n",
      "------------\n",
      "Epoch 19\n",
      "Training Loss: 0.0024; Training Acc: 100.0000\n",
      "Test Loss: 0.0568; Test Acc: 98.2372\n",
      "------------\n",
      "Epoch 20\n",
      "Training Loss: 0.0022; Training Acc: 99.9983\n",
      "Test Loss: 0.0561; Test Acc: 98.3574\n",
      "------------\n",
      "\n",
      "Training complete in 6m 5s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-3\n",
    "optimizer = optim.SGD(model_asy.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "#Perform the training \n",
    "train_model(model_asy, optimizer, criterion, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train another model which uses the sparse linear module along with this activation. As mentioned before, the learning rate is an order of magnitude higher than the linear module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_asy_sparse = nn.Sequential(\n",
    "\tFlatten(),\n",
    "\tsl.SparseLinear(784, 2000),\n",
    "    nn.LayerNorm(2000),\n",
    "    asy.ActivationSparsity(),\n",
    "    sl.SparseLinear(2000, 10),\n",
    ")\n",
    "model_asy_sparse = model_asy_sparse.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2218; Training Acc: 93.2080\n",
      "Test Loss: 0.1237; Test Acc: 96.1138\n",
      "------------\n",
      "Epoch 2\n",
      "Training Loss: 0.0960; Training Acc: 97.0318\n",
      "Test Loss: 0.0965; Test Acc: 97.0954\n",
      "------------\n",
      "Epoch 3\n",
      "Training Loss: 0.0670; Training Acc: 97.9172\n",
      "Test Loss: 0.0930; Test Acc: 97.1855\n",
      "------------\n",
      "Epoch 4\n",
      "Training Loss: 0.0501; Training Acc: 98.4075\n",
      "Test Loss: 0.0898; Test Acc: 97.2556\n",
      "------------\n",
      "Epoch 5\n",
      "Training Loss: 0.0391; Training Acc: 98.7777\n",
      "Test Loss: 0.0747; Test Acc: 97.7163\n",
      "------------\n",
      "Epoch 6\n",
      "Training Loss: 0.0303; Training Acc: 99.0261\n",
      "Test Loss: 0.0824; Test Acc: 97.5661\n",
      "------------\n",
      "Epoch 7\n",
      "Training Loss: 0.0234; Training Acc: 99.2596\n",
      "Test Loss: 0.0777; Test Acc: 97.7764\n",
      "------------\n",
      "Epoch 8\n",
      "Training Loss: 0.0173; Training Acc: 99.4981\n",
      "Test Loss: 0.0848; Test Acc: 97.4960\n",
      "------------\n",
      "Epoch 9\n",
      "Training Loss: 0.0138; Training Acc: 99.6398\n",
      "Test Loss: 0.0780; Test Acc: 97.8766\n",
      "------------\n",
      "Epoch 10\n",
      "Training Loss: 0.0096; Training Acc: 99.7699\n",
      "Test Loss: 0.0840; Test Acc: 97.7764\n",
      "------------\n",
      "Epoch 11\n",
      "Training Loss: 0.0083; Training Acc: 99.8066\n",
      "Test Loss: 0.0860; Test Acc: 97.8365\n",
      "------------\n",
      "Epoch 12\n",
      "Training Loss: 0.0068; Training Acc: 99.8616\n",
      "Test Loss: 0.0851; Test Acc: 97.8866\n",
      "------------\n",
      "Epoch 13\n",
      "Training Loss: 0.0049; Training Acc: 99.9200\n",
      "Test Loss: 0.0787; Test Acc: 98.0369\n",
      "------------\n",
      "Epoch 14\n",
      "Training Loss: 0.0033; Training Acc: 99.9666\n",
      "Test Loss: 0.0841; Test Acc: 98.0168\n",
      "------------\n",
      "Epoch 15\n",
      "Training Loss: 0.0025; Training Acc: 99.9783\n",
      "Test Loss: 0.0845; Test Acc: 97.8866\n",
      "------------\n",
      "Epoch 16\n",
      "Training Loss: 0.0022; Training Acc: 99.9850\n",
      "Test Loss: 0.0857; Test Acc: 98.0469\n",
      "------------\n",
      "Epoch 17\n",
      "Training Loss: 0.0017; Training Acc: 99.9933\n",
      "Test Loss: 0.0850; Test Acc: 98.1270\n",
      "------------\n",
      "Epoch 18\n",
      "Training Loss: 0.0013; Training Acc: 99.9967\n",
      "Test Loss: 0.0890; Test Acc: 98.0268\n",
      "------------\n",
      "Epoch 19\n",
      "Training Loss: 0.0012; Training Acc: 99.9967\n",
      "Test Loss: 0.0891; Test Acc: 98.1370\n",
      "------------\n",
      "Epoch 20\n",
      "Training Loss: 0.0010; Training Acc: 99.9983\n",
      "Test Loss: 0.0882; Test Acc: 98.0669\n",
      "------------\n",
      "\n",
      "Training complete in 6m 24s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-2\n",
    "optimizer = optim.SGD(model_asy_sparse.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "#Perform the training \n",
    "train_model(model_asy_sparse, optimizer, criterion, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training very wide and sparse models <a name=\"big\"></a>\n",
    "\n",
    "The main advantage of utilizing sparse tensors is that it enables us to train very wide models. Below we demonstrate an example of such a model. Of course, it is just a demonstration and the key take away is that we can build these huge models for more complex tasks where the benefits would be more viable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.sc1 = sl.SparseLinear(10 * 28 * 28, 50000, sparsity=0.999)\n",
    "        self.sc2 = sl.SparseLinear(50000, 50000, sparsity=0.999)\n",
    "        self.sc3 = sl.SparseLinear(50000, 50000, sparsity=0.999)\n",
    "        self.sc4 = sl.SparseLinear(50000, 50000, sparsity=0.999)\n",
    "        self.sc5 = sl.SparseLinear(50000, 50000, sparsity=0.999)\n",
    "        \n",
    "        self.input_scaling = nn.Parameter(torch.ones(10 * 28 * 28))\n",
    "        self.input_shifting = nn.Parameter(torch.zeros(10 * 28 * 28))\n",
    "        self.ln1 = nn.LayerNorm(50000)\n",
    "        self.ln2 = nn.LayerNorm(50000)\n",
    "        self.ln3 = nn.LayerNorm(50000)\n",
    "        self.ln4 = nn.LayerNorm(50000)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.repeat_interleave(x, 10, dim=1)\n",
    "        x = self.input_scaling * x + self.input_shifting\n",
    "        x = F.relu(self.ln1(self.sc1(x)))\n",
    "        x = F.relu(self.ln2(self.sc2(x)))\n",
    "        x = F.relu(self.ln3(self.sc3(x)))\n",
    "        x = F.relu(self.ln4(self.sc4(x)))\n",
    "        x = self.sc5(x)\n",
    "        x = x.view(x.shape[0], -1, 10).sum(dim=1)  # sum 5000 outputs per class\n",
    "        return x\n",
    "\n",
    "sparse_big = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.1993; Training Acc: 93.9168\n",
      "Test Loss: 0.1317; Test Acc: 95.9034\n",
      "------------\n",
      "Epoch 2\n",
      "Training Loss: 0.0675; Training Acc: 98.0023\n",
      "Test Loss: 0.0806; Test Acc: 97.5160\n",
      "------------\n",
      "Epoch 3\n",
      "Training Loss: 0.0318; Training Acc: 99.2663\n",
      "Test Loss: 0.0723; Test Acc: 97.6863\n",
      "------------\n",
      "Epoch 4\n",
      "Training Loss: 0.0167; Training Acc: 99.7532\n",
      "Test Loss: 0.0630; Test Acc: 97.9968\n",
      "------------\n",
      "Epoch 5\n",
      "Training Loss: 0.0088; Training Acc: 99.9483\n",
      "Test Loss: 0.0609; Test Acc: 98.0970\n",
      "------------\n",
      "Epoch 6\n",
      "Training Loss: 0.0056; Training Acc: 99.9917\n",
      "Test Loss: 0.0584; Test Acc: 98.2272\n",
      "------------\n",
      "Epoch 7\n",
      "Training Loss: 0.0041; Training Acc: 100.0000\n",
      "Test Loss: 0.0587; Test Acc: 98.1771\n",
      "------------\n",
      "Epoch 8\n",
      "Training Loss: 0.0033; Training Acc: 100.0000\n",
      "Test Loss: 0.0583; Test Acc: 98.1671\n",
      "------------\n",
      "Epoch 9\n",
      "Training Loss: 0.0028; Training Acc: 100.0000\n",
      "Test Loss: 0.0587; Test Acc: 98.1370\n",
      "------------\n",
      "Epoch 10\n",
      "Training Loss: 0.0024; Training Acc: 100.0000\n",
      "Test Loss: 0.0581; Test Acc: 98.1671\n",
      "------------\n",
      "Epoch 11\n",
      "Training Loss: 0.0021; Training Acc: 100.0000\n",
      "Test Loss: 0.0580; Test Acc: 98.1771\n",
      "------------\n",
      "Epoch 12\n",
      "Training Loss: 0.0019; Training Acc: 100.0000\n",
      "Test Loss: 0.0577; Test Acc: 98.2272\n",
      "------------\n",
      "Epoch 13\n",
      "Training Loss: 0.0017; Training Acc: 100.0000\n",
      "Test Loss: 0.0577; Test Acc: 98.2372\n",
      "------------\n",
      "Epoch 14\n",
      "Training Loss: 0.0016; Training Acc: 100.0000\n",
      "Test Loss: 0.0586; Test Acc: 98.1771\n",
      "------------\n",
      "Epoch 15\n",
      "Training Loss: 0.0014; Training Acc: 100.0000\n",
      "Test Loss: 0.0582; Test Acc: 98.1971\n",
      "------------\n",
      "Epoch 16\n",
      "Training Loss: 0.0013; Training Acc: 100.0000\n",
      "Test Loss: 0.0582; Test Acc: 98.2372\n",
      "------------\n",
      "Epoch 17\n",
      "Training Loss: 0.0012; Training Acc: 100.0000\n",
      "Test Loss: 0.0582; Test Acc: 98.2071\n",
      "------------\n",
      "Epoch 18\n",
      "Training Loss: 0.0012; Training Acc: 100.0000\n",
      "Test Loss: 0.0584; Test Acc: 98.1971\n",
      "------------\n",
      "Epoch 19\n",
      "Training Loss: 0.0011; Training Acc: 100.0000\n",
      "Test Loss: 0.0582; Test Acc: 98.2272\n",
      "------------\n",
      "Epoch 20\n",
      "Training Loss: 0.0010; Training Acc: 100.0000\n",
      "Test Loss: 0.0583; Test Acc: 98.1871\n",
      "------------\n",
      "\n",
      "Training complete in 39m 29s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-5\n",
    "optimizer = optim.SGD(sparse_big.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "train_model(sparse_big, optimizer, criterion, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we demonstrated the `SparseLinear` layer. From a user's perspective, it is very similar to PyTorch's `Linear` layer. We also showed extra features namely user-defined sparsity, dynamic sparsity, small-world connectivity, and activation sparsity. \n",
    "\n",
    "Our experiments showed that even with a huge reduction in parameters, we were able to achieve a performance similar to that of massively parameterised layers. \n",
    "\n",
    "We hope this excites and enables people to build highly scalable sparse networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](https://media.giphy.com/media/L0O3TQpp0WnSXmxV8p/giphy.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
